\documentclass{scrartcl}

\usepackage{cite}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

%\usepackage{algorithmic,algorithm}
\usepackage{amsmath,amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}

\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\begin{document}

\section{Introduction}

The EU Statistics on Income and Living Conditions (EU-SILC), launched 2004, represents a European standardized survey to generate comparative measures of poverty and social exclusion among the EU Member States. The survey is conducted annually in each country with a rotating panel design of 4 waves \citep[see][]{vebe2006}. In combination with a harmonized survey a set of common indicators, the so called Laeken indicators, was adopted for the countries of the EU \citep[see][]{atkinson2002}.
\newline
Due to sampling design and sample size the EU-SILC delivers qualitatively high well-being indicators at national or NUTS1 level, but usually lacks the capability to do the same for regional indicators, on for example NUTS2 or NUTS3 level. Due to the need of regional indicators for policy makers many methods have already been developed that aim to calculate statistical significant indicators on lower NUTS levels.\citep{gigaleva2012,povmap}
Many of these methods are based on models for small area estimation, pooling the data for consecutive years or applying a jack knife procedure for consecutive years and taking the mean over the jackknife replicates. for variance estimation. Others also use administrative data to impute the variable of interest, given a specialized, model onto a large data set, like the population census. Especially the last was has the downside of being very country specific and the risk of delivering estimates with high variance. Furthermore the use of administrative data also brings with it the problem of timeliness of the information in the administrative data.

In this work we present a method for estimating statistically significant estimates on lower NUTS levels using multiple years of EU-SILC in combination with bootstrapping techniques. In contrast to jack knife replicates, the bootstrap replicates yield consistent estimates for variance even for non-smooth estimators like the median. Due to the nature of our method it can easily be applied to EU-SILC data of any county given the constraint that EU-SILC data for at least 3 consecutive years is available. Furthermore the households in EU-SILC must be linked over the years through a household ID to ensure the applicability of our method.


\section{Methodology}
In the following we present the used methodology that is applied on multiple consecutive years of EU-SILC data for one country. The methodology contains the following steps, in this order

\begin{enumerate}
\item Draw $B$ bootstrap replicates from EU-SILC data for each year $y_t$, $t=1,\ldots,n_y$ separately. Since EU-SILC has a rotating panel design the bootstrap replicate of a household is carried forward through the years. That is, the bootstrap replicate of a household in the follow-up years is set equal to the bootstrap replicate of the same household when it first enters EU-SILC.
\item Multiply each set of bootstrap replicates by the sampling weights to obtain uncalibrated bootstrap weights and calibrate each of the uncalibrated bootstrap weights using iterative proportional fitting.
\item Estimate the point estimate of interest $\theta$, for each year and each calibrated bootstrap weight to obtain $\tilde{\theta}^{(i,y_t)}$, $t=1,\ldots,n_y$, $i=1,\ldots,B$. For fixed $y_t$ apply a filter with equal weights for each $i$ on $\tilde{\theta}^{(i,y^*)}$, $y^*\in \{y_{t-1},y_{t},y_{t+1}\}$ , to obtain $\tilde{\theta}^{(i,y_t)}$. Estimate the variance of $\theta$ using the distribution of $\tilde{\theta}^{(i,y_t)}$.
\end{enumerate}

\subsection{Bootstrapping}
Bootstrapping has long been around and used widely to estimate confidence intervals and standard errors of point estimates.\citep{efron1979}
Given a random sample $(X_1,\ldots,X_n)$ drawn from an unknown distribution $F$ the distribution of a point estimate $\theta(X_1,\ldots,X_n;F)$ can in many cases not be determined analytically. However when using bootstrapping one can simulate the distribution of $\theta$.

Let $s_{(.)}$ be a bootstrap sample, e.g. drawing $n$ observations with replacement from the sample $(X_1,\ldots,X_n)$, then one can estimate the standard deviation of $\theta$  using $B$ bootstrap samples through
\begin{align*}
  sd(\theta) = \sqrt{\frac{1}{B-1}\sum\limits_{i=1}^B (\theta(s_i)-\overline{\theta})^2} \quad,
\end{align*}
with $\overline{\theta}:=\frac{1}{B}\sum\limits_{i=1}^B\theta(s_i)$ as the sample mean over all bootstrap samples.

In context of sample surveys with sampling weights one can use bootstrapping to calculate so called bootstrap weights. These are computed via the bootstrap samples $s_{i}$, $i=1,\ldots,B$, where for each $s_{i}$ every unit of the original sample can appear $0$- to $m$-times. With $f_j^{i}$ as the frequency of occurrence of observation $j$ in bootstrap sample $s_i$ the uncalibrated bootstrap weights $\tilde{b}_{j}^{i}$ are defined as:

\begin{align*}
  \tilde{b}_{j}^{i} = f_j^{i} w_j \quad,
\end{align*}

with $w_j$ as the calibrated sampling weight of the original sample.
Using iterative proportional fitting procedures one can recalibrate the bootstrap weights $\tilde{b}_{j}^{.}$, $j=1,\ldots,B$ to get the adapted or calibrated bootstrap weights $b_j^i$, $j=1,\ldots,B$.

\subsubsection{Rescaled Bootstrap}
Since EU-SILC is a stratified sample without replacement drawn from a finite population the naive bootstrap procedure, as described above, does not take into account the heterogeneous inclusion probabilities of each sample unit. Thus it will not yield satisfactory results. Therefore we will use the so called rescaled bootstrap procedure introduced and investigated by \citep{raowu1988}. The bootstrap samples are selected without replacement and do incorporate the stratification as well as clustering on multiple stages (see \citep{chipprest2007},\citep{prest2009}).\\
\newline

For simplistic reasons we will only describe the rescaled bootstrap procedure for a two stage stratified sampling design. For more details on a general formulation please see \citep{prest2009}.

\paragraph{Sampling design}
Consider the finite population $U$ which is divided into $H$ non-overlapping strata $\bigcup\limits_{h=1,\ldots,H} U_h = U$, of which each strata $h$ contains of $N_h$ clusters. For each strata $h$, $C_{hc}$, $c=1,\ldots,n_h$ clusters are drawn, containing $N_{hc}$ households. Furthermore in each cluster $C_{hc}$ of each strata $h$ simple random sampling is performed to select a set of households $Y_{hcj}$, $j=1,\ldots,n_{hc}$.

\paragraph{Bootstrap procedure}
In contrast to the naive bootstrap procedure where for a stage, containing $n$ sampling units, the bootstrap replicate is obtained by drawing $n$ sampling units with replacement, for the rescaled bootstrap procedure $n^*=\left\lfloor\frac{n}{2}\right\rfloor$ sampling units are drawn without replacement. Given a value $x$, $\lfloor x\rfloor$ denotes the largest integer smaller than $x$, whereas $\lceil x\rceil$ denotes the smallest integer lager then $x$. \citep{chipprest2007} have shown that the choice of either $\left\lfloor\frac{n}{2}\right\rfloor$ or $\left\lceil\frac{n}{2}\right\rceil$ is optimal for bootstrap samples without replacement, although $\left\lfloor\frac{n}{2}\right\rfloor$ has the desirable property that the resulting uncalibrated bootstrap weights will never be negative.\\
\newline
At the first stage the $i$-th bootstrap replicate, $f^{i,1}_{hc}$, for each cluster $C_{hc}$,$c=1,\ldots,n_h$, belonging to strata $h$, is defined by

\begin{align*}
  f^{i,1}_{hc} =& 1-\lambda_h+\lambda_h\frac{n_h}{n_h^*}\delta_{hc} \quad\quad \forall c \in \{1,\ldots,n_h\} \\
  \intertext{with}
  n_h^* =& \left\lfloor\frac{n_h}{2}\right\rfloor \\
  \lambda_h =& \sqrt{\frac{n_h^*(1-\frac{n_h}{N_h})}{n_h-n_h^*}} \quad ,
\end{align*}

where $\delta_{hc}=1$ if cluster $c$ is selected in the sub-sample of size $n_h^*$ and 0 otherwise.\\
\noindent
The $i$-th bootstrap replicate at the second stage, $f^{i,2}_{hcj}$, for each household $Y_{hcj}$, $j=1,\ldots,n_{hc}$, belonging to cluster $c$ in strata $h$ is defined by

\begin{align*}
  f^{i,2}_{hcj} =& f^{i,1}_{hc} - \lambda_{hc}\sqrt{\frac{n_h}{n_h^*}}\delta_{hc}\left[\frac{n_{hc}}{n_{hc}^*}\delta_{hcj}-1\right] \quad\quad \forall c \in \{1,\ldots,n_h\}\\
  \intertext{with}
  n_{hc}^* =& \lfloor\frac{n_{hc}}{2}\rfloor \\
  \lambda_{hc} =& \sqrt{\frac{n_{hc}^*N_h(1-\frac{n_{hc}}{N_{hc}})}{n_{hc}-n_{hc}^*}} \quad ,
\end{align*}

where $\delta_{hcj}=1$ if household $j$ is selected in the sub sample of size $n_{hc}^*$ and 0 otherwise.\\

\paragraph{Single PSUs}
When dealing with multistage sampling designs the issue of single PSUs, e.g. a single response unit is present at a stage or in a strata, can occur. When applying bootstrapping procedures these single PSUs can lead to a variety of issues. For the methodology proposed in this work we combined single PSUs at each stage with the next smallest strata or cluster, before applying the bootstrap procedure.

\paragraph{Taking bootstrap replicates forward}
The bootstrap procedure above is applied on the EU-SILC data for each year $y_t$, $t=1,\ldots,n_y$ separately. Since EU-SILC is a yearly survey with rotating penal design the $i$-th bootstrap replicate at the second stage, $f^{i,2}_{hcj}$, for a household $Y_{hcj}$ is taken forward until the household $Y_{hcj}$ drops out of the sample. That is, for the household $Y_{hcj}$, which enters EU-SILC at year $y_1$ and drops out at year $y_{\tilde{t}}$, the bootstrap replicates for the years $y_2,\ldots,y_{\tilde{t}}$ are set to the bootstrap replicate of the year $y_1$.

\paragraph{Split households}
Due to the rotating penal design so called split households can occur. For a household participating in the EU-SILC survey it is possible that one or more residents move to a new so called split household, which is followed up on in the next wave. To take this dynamic into account we extended the procedure of taking forward the bootstrap replicate of a household for consecutive waves of EU-SILC by taking forward the bootstrap replicate to the split household. That means, that also any new individuals in the split household will inherit this bootstrap replicate.\\
\newline
The following Tables illustrate the mechanism for taking bootstrap replicates forward es well as dealing with split households.
Consider a household that enters EU-SILC in the year 2013 and leaves after 2014, as show in Table \ref{tab:hh1}. For 2013 the household, HID=47500, contained 4 household members of which one household member, PID=4750003, moved to a new household in year 2014. Thus creating the split household, HID=47501, in year 2014.
The bootstrap procedure, since it is applied on each year separately, did select the household in 2013, e.g BOOTSTRAP.REP=2, but did not select either HID=47500 or HID=47501 for the year 2014, e.g BOOTSTRAP.REP=0.

<<echo=FALSE,include=FALSE,cache=TRUE>>=
library(data.table)
library(surveysd)
library(mountSTAT)
library(knitr)

dat <- fread(paste0(mountO(),"/B/Datenaustausch/NETSILC3/udb_short_new.csv"))
dat[,RB050:=gsub(",","\\.",RB050)]
dat[,RB050:=as.numeric(RB050)]

dat_es <- dat[rb020=="ES"][!duplicated(paste(RB030,RB010,sep="_"))]

dat_es[,DB050_old:=DB050]
dat_es <- dat_es[!is.na(DB030)]

dat_es <- generate.HHID(dat_es)
dat_es[,uniqueN(DB030_orig),by=list(DB030,RB010)][V1>1&RB010==2014]
dat_split <- dat_es[DB030_orig%in%c(47500,47501)&RB010%in%c(2013,2014),.(YEAR=RB010,PID=as.character(RB030),HID=DB030_orig)]
dat_split[YEAR==2013,BOOTSTRAP.REP:=2]
dat_split[YEAR==2014,BOOTSTRAP.REP:=0]
@

<<hh1,echo=FALSE, results='asis'>>=
kable(dat_split,format="latex",booktabs=TRUE,align=c("l","l","l","r"),caption = "Household with 4 members in year 2013 and 3 members plus 1 split household in year 2014.")
@

<<hh2,echo=FALSE, results='asis'>>=
dat_split[,BOOTSTRAP.REP:=2]
kable(dat_split,format="latex",booktabs=TRUE,align=c("l","l","l","r"),caption = "Bootstrap replicates taken forward to the next year as well as to the split household.")
@

Following our rule for taking bootstrap replicates forward and dealing with split households the bootstrap replicates for the year 2014 will be set equal to the bootstrap replicates for 2013. This is also true for the split household since it was created through the household member PID=4750003, which moved out of household HID=47500 between 2013 and 2014. Every Household member in of HID=47501 will therefore inherit the bootstrap replicate which household member PID=4750003 had in the year 2013.
Table \ref{tab:hh2} shows the bootstrap replicates after they have been taken forward from 2013.

Taking bootstrap replicates forward as well as considering split households ensures that bootstrap replicates are more comparable in structure with the actual design of EU-SILC.

\paragraph{Uncalibrated bootstrap weights}
Using the $i$-th bootstrap replicates at the second stage one can calculate the $i$-th uncalibrated bootstrap weights $b_{hcj}^{i}$ for each household $Y_{hcj}$ in cluster $c$ contained in strata $h$ by

\begin{align*}
  \tilde{b}_{hcj}^{i} =& f^{i,2}_{hcj} w_{hcj} \quad,
\end{align*}
where $w_{hcj}$ corresponds to the original household weight contained in the sample.\\

For ease of readability we will drop the subindices regarding strata $h$ and cluster $c$ for the following sections, meaning that the $j$-th household in cluster $c$ contained in strata $h$, $Y_{hcj}$, will now be denoted as the $j$-th household, $Y_{j}$, where $j$ is the position of the household in the data. In accordance to this the $i$-th uncalibrated bootstrap replicates for household $j$ are thus denoted as $\tilde{b}_j^{i}$ and the original household weight as $w_j$.


\subsection{Iterative proportional fitting (IPF)}
The uncalibrated bootstrap weights $\tilde{b}_j^{i}$ computed through the rescaled bootstrap procedure yields population statistics that differ from the known population margins of specified sociodemographic variables for which the base weights $w_j$ have been calibrated. To adjust for this the bootstrap weights $\tilde{b}_{j}^{i}$ can be recalibrated using iterative proportional fitting as described in \citep{mekogu2016}.\\

Let the original weight $w_{j}$ be calibrated for $n=n_P+n_H$ sociodemographic variables which are divided into the sets $\mathcal{P}:=\{p_{c}, c=1 \ldots,n_P\}$ and $\mathcal{H}:=\{h_{c}, c=1 \ldots,n_H\}$. $\mathcal{P}$ and $\mathcal{H}$ correspond to personal, for example gender or age, or household variables, like region or households size, respectively. Each variable in either $\mathcal{P}$ or $\mathcal{H}$ can take on $P_{c}$ or $H_{c}$ values with and $N^{p_c}_v$, $v=1,\ldots,P_c$, or $N^{h_c}_v$, $v=1,\ldots,H_c$, as the corresponding population margins. Starting with $k=0$ the iterative proportional fitting procedure is applied on each $\tilde{b}_j^{i}$, $i=1,\ldots, B$ seperately. The weights are first updated for personal and afterwards updated for household variables. If constraints regarding the populations margins are not met $k$ is raised by 1 and the procedure starts from the beginning. For the following denote as starting weight $\tilde{b}_j^{[0]}:=\tilde{b}_j^{i}$ for fixed $i$.

\subsubsection{Adjustment and trimming for $\mathcal{P}}
The uncalibrated bootstrap weight $\tilde{b}_j^{[(n+1)k+c-1]}$ for the $j$-th observation is iteratively multiplied by a factor so that the projected distribution of the population matches the respective calibration specification $N_{p_c}$, $c=1, \ldots,n_P$.
For each $c \in \left\{1, \ldots,n_P\right\}$ the calibrated weights against $N^{p_c}_v$ are computed as

\begin{align*}
  \tilde{b}_j^{[(n+1)k+c]} = {\tilde{b}_j}^{[(n+1)k+c-1]}\frac{N^{p_c}_v}{{\sum\limits_l} {\tilde{b}}_l^{[(n+1)k+c-1]}},
\end{align*}

where the summation in the denominator expands over all observations which have the same value as observation $j$ for the sociodemographic variable $p_c$.
If any weights $\tilde{b}_j^{[nk+c]}$ fall outside the range $\left[\frac{w_j}{4};4w_j\right]$ they will be recoded to the nearest of the two boundaries. The choice of the boundaries results from expert-based opinions and restricts the variance of which has a positive effect on the sampling error. This procedure represents a common form of weight trimming where very large or small weights are trimmed in order to reduce variance in exchange for a possible increase in bias ([@potter90],[@potter93]).

\subsubsection{Averaging weights within households}
Since the sociodemographic variables $p_1,\ldots,p_{n_c}$ include person-specific variables, the weights $\tilde{b}_j^{[nk+n_p]}$ resulting from the iterative multiplication can be unequal for members of the same household. This can lead to inconsistencies between results projected with household and person weights. To avoid such inconsistencies each household member is assigned the mean of the household weights. That is for each person $j$ in household $a$ with $h_a$ household members, the weights are defined by

\begin{align*}
  \tilde{b}_j^{[(n+1)k+n_p+1]} = \frac{{\sum\limits_{l\in a}} {\tilde{b}_l^{[(n+1)k+n_p]}}}{h_a}
\end{align*}

This can result in losing the population structure performed in the previouse subsection.

\subsubsection{Adjustment and trimming for $\mathcal{H}$}
After adjustment for individual variables the weights $b_j^{[nk+n_p+1]}$ are updated for the set of household variables $\mathcal{H}$ according to a household convergence constraint parameter $\epsilon_h$. The parameters $\epsilon_h$ represent the allowed deviation from the population margins using the weights $b_j^{[nk+n_p+1]}$ compared to $N^{h_c}_v$, $c=1,\ldots,n_H$, $v=1,\ldots,H_c$.
The updated weights are computed as

\begin{align*}
  b_j^{[(n+1)k+n_p+c+1]} =
  \begin{cases}
    b_j^{[(n+1)k+n_p+1]}\frac{N^{h_c}_v}{\sum\limits_{l} b_l^{[(n+1)k+n_p+1]}} \quad \text{if } \sum\limits_{l} b_j^{[(n+1)k+n_p+1]} \notin ((1-0.9\epsilon_h)N^{h_c}_v,(1+0.9\epsilon_h)N^{h_c}_v) \\
    b_j^{[(n+1)k+n_p+1]} \quad \text{otherwise}
  \end{cases}
\end{align*}

with the summation in the denominator ranging over all households $l$ which take on the same values for $h_c$ as observation $j$. As described in the previous subsection the new weight are recoded if they exceed the interval $[\frac{w_j}{4};4w_j]$ and set to the upper or lower bound, depending of $b_j^{[(n+1)k+n_p+c+1]}$ falls below or above the interval respectively.

\subsubsection{Convergence}
For each adjustment and trimming step the factor $\frac{N^{(.)}_v}{\sum\limits_{l} b_l^{[(n+1)k+j]}}$, $j\in \{1,\ldots,n+1\}\backslash \{n_p+1\}$, is checked against convergence constraints for households, $\epsilon_h$, or personal variables $\epsilon_p$, where $(.)$ corresponds to either a household or personal variable.
To be more precise for variables in $\mathcal{P}$ the constraints

\begin{align*}
\frac{N^{p_c}_v}{{\sum\limits_l} {\tilde{b}}_l^{[(n+1)k+j]}} \in ((1-\epsilon_p)N^{p_c}_v,(1+\epsilon_p)N^{p_c}_v)
\end{align*}
and for variables in $\mathcal{H}$ the constraints

\begin{algin*}
\frac{N^{h_c}_v}{{\sum\limits_l} {\tilde{b}}_l^{[(n+1)k+j]}} \in ((1-\epsilon_h)N^{h_c}_v,(1+\epsilon_h)N^{h_c}_v)
end{align*}

are verified, where the sum in the denominator expands over all observations which have the same value for variables $h_c$ or $p_c$.
If these contraints hold true the algorithm reaches convergence, otherwise $k$ is raised by 1 and the procedure repeats itself.

The above described calibration procedure is applied on each year $y_t$ of EU-SILC separately, $t=1,\ldots n_y$, thus resulting in so called calibrated bootstrap sample weights $b_{j}^{(i,{y_t})}$, $i=1,\ldots,B$ for each year $y$ and each household $j$.

\subsection{Variance estimation}
Applying the previously described algorithms to EU-SILC data for multiple consecutive years $y_t$, $t=1,\ldots n_y$, yields calibrated bootstrap sample weights $b_{j}^{(i,{y_t})}$ for each year $y_t$. Using the calibrated bootstrap sample weights it is straight forward to compute the standard error of a point estimate $\theta(\textbf{X}^{y_t},\textbf{w}^{y_t})$ for year $y_t$ with $\textbf{X}^{y_t}=(X_1^{y_t},\ldots,X_n^{y_t})$ as the vector of observations for the variable of interest in the survey and $\textbf{w}^{y_t}=(w_1^{y_t},\ldots,w_n^{y_t}$ as the corresponding weight vector, with

\begin{align*}
  sd(\theta) =& \sqrt{\frac{1}{B-1}\sum\limits_{i=1}^B (\theta^{(i,y_t)}-\overline{\theta^{(.,y_t)}})^2} \\
\intertext{with}
  \overline{\theta^{(.,y_t)}} =& \frac{1}{B}\sum\limits_{i=1}^B\theta^{(i,y_t)} \quad,
\end{align*}
where $\theta^{(i,y_t)}:=\theta(\textbf{X}^{y_t},\textbf{b}^{(i,{y_t})})$ is the estimate of $\theta$ in the year $y_t$ using the $i$-th vector of calibrated bootstrap weights.

As already mentioned the standard error estimation for indicators in EU-SILC yields high quality results for NUTS1 or country level. When estimation indicators on regional or other sub-aggregate levels one is confronted with point estimates yielding high variance.\\

To overcome this issue we propose to estimate $\theta$ for 3, consecutive years using the calibrated bootstrap weights, thus calculating $\{\theta^{(i,y_{t-1})},\theta^{(i,y_t)},\theta^{(i,y_{t+1})}\}$, $i=1,\ldots,B$.
For fixed $i$ one can apply a filter with equal filter weights on the time series $\{\theta^{(i,y_{t-1})},\theta^{(i,y_t)},\theta^{(i,y_{t+1})}\}$ to create $\tilde{\theta}^{(i,y_t)}$

\begin{align*}
  \tilde{\theta}^{(i,y_t)} = \frac{1}{3}\left[\theta^{(i,y_{t-1})}+\theta^{(i,y_t)}+\theta^{(i,y_{t+1})}\right] \quad .
\end{align*}


Doing this for all $i$, $i=1,\ldots,B$, yields $\tilde{\theta}^{(i,y_t)}$, $i=1,\ldots,B$. The standard error of $\theta$ can then be estimated with

\begin{align*}
  sd(\theta) =& \sqrt{\frac{1}{B-1}\sum\limits_{i=1}^B (\tilde{\theta}^{(i,y_t)}-\overline{\tilde{\theta}^{(.,y_t)}})^2}\\
  \intertext{with}
  \overline{\tilde{\theta}^{(.,y_t)}}=&\frac{1}{B}\sum\limits_{i=1}^B\tilde{\theta}^{(i,y_t)} \quad.
\end{align*}

Applying the filter over the time series of estimated $\theta^{(i,y_t)}$ leads to a reduction of variance for $\theta$ since the filter reduces the noise in $\{\theta^{(i,y_{t-1})},\theta^{(i,y_t)},\theta^{(i,y_{t+1})}\}$ and thus leading to a more narrow distribution for $\tilde{\theta}^{(i,y_t)}$.

It should also be noted that estimating indicators from a survey with rotating panel design is in general not straight forward because of the high correlation between consecutive years. However with our approach to use bootstrap weights, which are independent from each other, we can bypass the cumbersome calculation of various correlations, and apply them directly to estimate the standard error.\\
\citep{silcstudy} showed that using the proposed method on EU-SILC data for Austria the reduction in resulting standard errors corresponds in a theoretical increase in sample size by about 25$\%$. Furthermore this study compared this method to the use of small area estimation techniques and on average the use of bootstrap sample weights yielded more stable results.\\

\subsubsection{Improvement of 3 year mean}
In the following we will show the improvement of precision of using the mean over several years on the UDB data of EU-SILC for Spain and Austria.
We created 250 bootstrap replicates and calibrated for Spain each of the replicates for the variable DB040, household size and gender by age group. For Austria we calibrated by DB040 and RB090. After that the weighted ratio for AROPE was estimated, including it's standard error for each DB040 by DB100 for each year as well as using 3 consecutive years.\\
Figure \ref{fig:compareMean} shows for the results of this calculation for the year 2015 and the mean over years 2014, 2015 and 2016.
The identity line corresponds to the estimates using a single year as basis for standard error estimation and the points show the results for using the years 2014 to 2016. The dotted line represents the 25$\%$ improvement that is to be expected for using the mean over 3 consecutive years, as shown by \citep{silcstudy}. It is clear to see that using the UDB the improvement of the the point estimate for standard error by about 25$\%$, when using 3 consecutive years, holds true as well.

<<test, include=FALSE,cache=TRUE,cache.lazy = FALSE>>=
library(data.table)
library(mountSTAT)
library(surveysd)
pfad_meth <- mountWinShare("DatenREG","REG_METHODIK","meth")[1]
dat_boot_calib <- fread(paste0(mountO(),"/B/Datenaustausch/NETSILC3/udb_ES_calib.csv"))

res_es <- calc.stError(dat=dat_boot_calib,weights="RB050",year="RB010",b.weights=paste0("w",1:250),
               var="arose",cross_var=list(c("DB040","DB100")))

dat_boot_calib_at <- fread(paste0(mountO(),"/B/Datenaustausch/NETSILC3/udb_AT_calib.csv"),select=1:300)

res_at <- calc.stError(dat=dat_boot_calib_at,weights="RB050",year="RB010",b.weights=paste0("w",1:250),
               var="arose",cross_var=list(c("DB040","DB100")))
select.year <- c("2015","2014_2015_2016")

res_es <- res_es$Estimates[!is.na(DB040)&!is.na(DB100)&RB010%in%select.year]
res_at <- res_at$Estimates[!is.na(DB040)&!is.na(DB100)&RB010%in%select.year]

res_all <- rbind(res_es,res_at,use.names=TRUE)
res_all <- dcast(res_all,DB040+DB100~RB010,value.var = "stE_arose")
setnames(res_all,c("2015","2014_2015_2016"),c("Identity","Mean over 3 years"))
res_all[,`25% below`:=Identity*.75]
res_all[,State:=substr(DB040,1,2)]
res_all[,xaxis:=Identity]

res_all <- melt(res_all,id.vars=c("DB040","State","DB100","Mean over 3 years","xaxis"),measure.vars = c("Identity","25% below"))
@

<<compareMean, echo=FALSE,warning=FALSE,fig.cap="Results for setup 1 regarding the sensitivity analysis on standard error estimations.">>=
library(ggplot2)
ggplot(res_all,aes(xaxis,value))+
  geom_line(aes(linetype=variable))+
  geom_point(aes(y=`Mean over 3 years`,color=State))+xlab("Value of Standard Error")+ylab("")+
  scale_linetype_discrete(name="")+
  facet_grid(State~.,scales="free")
@

To conclude the above presented methodology represents a reasonable approach for lowering the variance of point estimates if the estimates stay relatively stable over time. However for point estimates which change trend over time or exhibit seasonal or cyclical behavior this methodology can not be recommended. At least not with applying a filter with equal filter weights.

\subsection{R-Package vardpoor}
For the work-package on NetSilc 2 a different methodology was proposed for estimating standard errors for EU-SILC by \citep{soton416829} and even implemented this methodology in the R-Package vardpoor, see \citep{vardpoor}. This approach uses the so called ultimate cluster approximation, where the variance between the sampling units at the first sampling stage is used as approximation for the total sampling variance (see \citep{saerndal1992model}). Using this approximation the standard error estimates are analytically calculated and if needed approximated through linearization (\citep{devilleClaude}).\\
To show that our proposed method is in accord to the methodology proposed in NetSilc 2 we applied both methods on UDB data of EU-SILC for Spain to estimate standard errors for the weighted ratio of AROPE on NUTS2-Level for the year 2016.\\
Table \ref{tab:compvard} shows the estimated standard errors for both methodologies. Our proposed methodology was run with 250 and 1000 bootstrap sample weights, columns
surveysd\_250 and surveysd\_1000, respectively. We see that for almost all standard errors the results for our methodology are slightly higher. This is to be expected since we do incorporate the complete sampling design in our calculations.

<<include=FALSE,cache=TRUE>>=
# DATA ES
library(data.table)
library(surveysd)
library(vardpoor)
library(mountSTAT)

dat <- fread(paste0(mountO(),"/B/Datenaustausch/NETSILC3/udb_short_new.csv"))
dat[,RB050:=gsub(",","\\.",RB050)]
dat[,RB050:=as.numeric(RB050)]

dat_es <- dat[rb020=="ES"][!duplicated(paste(RB030,RB010,sep="_"))]

dat_es[,DB050_old:=DB050]
dat_es <- dat_es[!is.na(DB030)]

# definiere strate für Spanien
for(i in 2013:2009){
  strat_lookup <- unique(na.omit(dat_es[RB010>=i,.(DB060,DB050_neu=DB050)]))

  dat_es_i <- dat_es[RB010==(i-1)]
  dat_es_i <- merge(dat_es_i,strat_lookup,by=c("DB060"),all.x=TRUE)
  dat_es_i[,DB050_neu:=na.omit(DB050_neu)[1],by=list(DB060)]

  na.group <- unique(dat_es_i[is.na(DB050_neu),.(DB040,DB050)])
  if(nrow(na.group)>0){
    setkeyv(dat_es_i,c("DB040","DB050"))
    choose.group <- dat_es_i[na.group,length(unique(na.omit(DB050_neu))),by=list(DB040,DB050)][V1==1,.(DB040,DB050)]
    dat_es_i[choose.group,DB050_neu:=na.omit(DB050_neu)[1],by=list(DB040,DB050)]
  }

  dat_es <- merge(dat_es,dat_es_i[,.(RB030,RB010,DB050_neu)],by=c("RB030","RB010"),all.x=TRUE)
  dat_es[RB010==(i-1),DB050:=DB050_neu]
  dat_es[,DB050_neu:=NULL]
}
#dat_es[is.na(db050)]
#dcast(dat_es[,sum(RB050),by=list(RB010,db050)],db050~RB010)

# Cluster aufteilen
# Anteil census section 35917

random_round <- function(x){
  set.seed(1234)
  x_off <- sum(x-floor(x))
  up_down <- rep(FALSE,length(x))
  if(x_off>0){
    up_down[1:x_off] <- TRUE
    up_down <- sample(up_down)
    x[up_down] <- ceiling(x[up_down])
    x[!up_down] <- floor(x[!up_down])
  }
  return(x)
}

strata <- dat_es[,.(STRATA_sum=sum(RB050[!duplicated(DB030)])),by=list(DB050,RB010)]
strata[,STRATA_ratio:=STRATA_sum/sum(STRATA_sum),by=RB010]
strata[,N.cluster:=random_round(STRATA_ratio*35917),by=RB010]
strata[,N.households:=STRATA_sum/N.cluster]

dat_es <- merge(dat_es,strata[,.(DB050,RB010,N.cluster,N.households)],by=c("DB050","RB010"))
dat_es[,IDd:=paste0("V",.I)]

# N_h <- dat_es[,sum(RB050[!duplicated(DB030)]),by=list(RB010,DB050)]
N_h <- unique(dat_es[,.(RB010,DB050,N=N.cluster)])
erg_vardpoor <- varpoord(Y="arose",H="DB050",PSU="DB060",Dom="DB040",ID_level1 = "DB060",N_h=N_h,
                 ID_level2 = "IDd",w_final="RB050",period="RB010",dataset=dat_es,type="linarpr")


# dat_boot_calib <- fread(paste0(mountO(),"/B/Datenaustausch/NETSILC3/udb_ES_calib.csv"))

dat_boot_calib[,arose_neu:=ifelse(arose==1,0,1)]
erg_survey_250 <- calc.stError(dat=copy(dat_boot_calib),weights="RB050",year="RB010",b.weights=paste0("w",1:250),
                           var="arose_neu",cross_var=list(c("DB040")))

erg_survey_1000 <- calc.stError(dat=copy(dat_boot_calib),weights="RB050",year="RB010",b.weights=paste0("w",1:1000),
                           var="arose_neu",cross_var=list(c("DB040")))


erg_comp <- merge(erg_vardpoor$all_result[RB010==2016,.(RB010,DB040,vardpoor=se)],
                  erg_survey_250$Estimates[RB010==2016,.(RB010, DB040, val_arose_neu,surveysd_250=stE_arose_neu)],
                  by=c("RB010","DB040"))

erg_comp <- merge(erg_comp,erg_survey_1000$Estimates[RB010==2016,.(RB010, DB040, surveysd_1000=stE_arose_neu)],
                  by=c("RB010","DB040"))
erg_comp <- erg_comp[,c(2,3,5,6),with=FALSE]
@

<<compvard,echo=FALSE, results='asis'>>=
kable(erg_comp,format="latex",booktabs=TRUE,caption=c("Standard error estimates with ultimate cluster approximation and bootstrapping."))
@


\subsection{Number of bootstrap replicates}
When using bootstrap replicates with survey data it is not always clear how many bootstrap replicates are needed to get stable results.
Obviously it is more beneficial to use too many bootstrap replicates instead of too little. Nevertheless using a very large number of bootstrap replicates on large data sets can heavily increase memory usage as well as run-time for the computation. To get a bit of insight on how many bootstrap replicates suffice for reasonable results we conducted a sensitivity analysis.

\subsubsection{Setup}
For the sensitivity analysis we used the UDB data of EU-SILC for Spain for the years 2008 till 2016. We created 1000 bootstrap replicates and calibrated each of the replicates for the variable DB040, household size and gender by age group. The estimate of interest was the weighted Ratio in $\%$ of AROPE for each year at a national level as well as the grouping DB040 by DB100. The sensitivity analysis was then conducted with two different approaches
\begin{enumerate}
  \item Starting with 50 bootstrap sample weight the standard error, 0.025 and 0.975 quantile were calculated. After that the next bootstrap sample weight was added and standard error, 0.025 and 0.975 quantile were calculated again. This was done until all 1000 bootstrap sample weights were used for the calculation.
  \item We sampled $i$ bootstrap sample weight from the original set of 1000 bootstrap sample weight and calculated standard error, 0.025 and 0.975 quantile of the point estimate. $i$ was chosen to be in $I:=\{i: i\in 50,100,150,\ldots,700,750,800\}$ and for each $k$ this procedure was done 100 times over.
\end{enumerate}

Calculating the estimates for year, DB040 and DB100 can in some cases yield very little observations in the data. Since we expect the behavior of estimates for standard error, 0.025 and 0.975 quantile the be depending on the number of observations used we divided the results into groups. The groups were $G:=\{g: g\in \{[0-50), [50,100), [100,250), [250,500), [500,1000), 1000+\}\}$, displaying the range for number of observations used from the data for each year and each year by DB040 and DB100.

\subsubsection{Results Setup 1}
Let $\hat{\sigma}^{(i)}_{(k,g)}$ be the $k$-th estimate for standard error using $i$ bootstrap sample weights in group $g$ which contains $n_g$ estimates, $g\in G$. We first calculated for each $g$ the absolute change, $\tilde{\sigma}^{(i)}_{(k,g)}$, for the point estimate
\begin{align*}
  \tilde{\sigma}^{(i)}_{(k,g)} = \hat{\sigma}^{(i+1)}_{(k,g)} - \hat{\sigma}^{(i)}_{(k,g)} \quad i=1,\ldots,999.
\end{align*}

Afterwards for each group $g$ and number of bootstrap sample weights $i$ we estimated the minimum, maximum and median over $\tilde{\sigma}^{(i)}_{(k,g)}$. In Figure \ref{fig:setup1SD} this minimum, maximum and median is displayed for each group and number of bootstrap sample weights used. The y axis was, for better readability, cut of below and above -1 and 1, respectively. The same calculations were done for the 0.025 and 0.975 quantile and the results are displayed in Figure \ref{fig:setup1p0025} and Figure \ref{fig:setup1p0975}.

<<include=FALSE,cache=TRUE>>=
library(data.table)
library(mountSTAT)
pfad_meth <- mountWinShare("DatenREG","REG_METHODIK","meth")[1]

load(paste0(pfad_meth,"/Gussenbauer/surveysd/Results_i1000_arose.RData"))

res <- rbindlist(res)
res <- res[RB010%in%dat_es[,unique(RB010)]&NumberWeights>=25]
res[,RB010:=as.numeric(RB010)]

num_obs <- rbindlist(list(dat_es[,.(N=sum(!duplicated(DB030))),by=RB010],dat_es[,.(N=sum(!duplicated(DB030))),by=list(RB010,DB040,DB100)]),use.names=TRUE,fill=TRUE)
num_obs[,GROUP:=cut(N,c(-Inf,50,100,250,500,1000,Inf,right=FALSE))]

res <- merge(res,num_obs,by=c("RB010","DB040","DB100"))
res[,GROUP_VAR:=.GRP,by=c("RB010","DB040","DB100")]

estim <- c("stE_arose","p0.025_arose","p0.975_arose")
main.lab <- c("Standard Error","0.025 Quantile","0.975 Quantile")
sel_var <- c("RB010","DB040","DB100","NumberWeights","GROUP_VAR","GROUP")
@


<<setup1SD, echo=FALSE,fig.cap="Results for setup 1 regarding the sensitivity analysis on standard error estimations.">>=
# standard error plot
library(data.table)
library(ggplot2)
  i <- 1
  dat_plot <- subset(res,select=c(sel_var,estim[i]))
  # berechne Veränderungsrate vom estimate über NumberWeights
  setkeyv(dat_plot,c("GROUP_VAR","NumberWeights"))
  dat_plot <- na.omit(dat_plot[,.(EST_CHANGE=diff(get(estim[i])),NumberWeights=NumberWeights[-1]),by=list(GROUP_VAR,GROUP)])
  dat_plot <- dat_plot[,.(Q=c("min","med","max"),quantile(EST_CHANGE,c(0,.5,1))),by=list(GROUP,NumberWeights)]
  dat_plot <- dcast(dat_plot,GROUP+NumberWeights~Q,value.var="V2")
  setnames(dat_plot,c("GROUP"),c("NumberObs"))

  p1 <- ggplot(dat_plot,aes(NumberWeights,med,group=NumberObs))+
    geom_line(aes(color=NumberObs),size=.25)+xlab("Number Bootstrap weights")+ylab("Min - Median - Max")+
    geom_ribbon(aes(ymin = min, ymax = max,fill=NumberObs,colour=NumberObs),linetype = 2, alpha= 0.1)+
    guides(color=guide_legend(title="Number of\nObservations"),
           fill=guide_legend(title="Number of\nObservations"))+
    coord_cartesian(ylim=c(1,-1))+
    facet_grid(NumberObs~.)+ggtitle(main.lab[i])

  plot(p1)
@


<<setup1p0025, echo=FALSE,fig.cap="Results for setup 1 regarding the sensitivity analysis on estimations for the 0.025 quantile.">>=
# p0.025 plot
library(data.table)
library(ggplot2)
  i <- 2
  dat_plot <- subset(res,select=c(sel_var,estim[i]))
  # berechne Veränderungsrate vom estimate über NumberWeights
  setkeyv(dat_plot,c("GROUP_VAR","NumberWeights"))
  dat_plot <- na.omit(dat_plot[,.(EST_CHANGE=diff(get(estim[i])),NumberWeights=NumberWeights[-1]),by=list(GROUP_VAR,GROUP)])
  dat_plot <- dat_plot[,.(Q=c("min","med","max"),quantile(EST_CHANGE,c(0,.5,1))),by=list(GROUP,NumberWeights)]
  dat_plot <- dcast(dat_plot,GROUP+NumberWeights~Q,value.var="V2")
  setnames(dat_plot,c("GROUP"),c("NumberObs"))

  p1 <- ggplot(dat_plot,aes(NumberWeights,med,group=NumberObs))+
    geom_line(aes(color=NumberObs),size=.25)+xlab("Number Bootstrap weights")+ylab("Min - Median - Max")+
    geom_ribbon(aes(ymin = min, ymax = max,fill=NumberObs,colour=NumberObs),linetype = 2, alpha= 0.1)+
    guides(color=guide_legend(title="Number of\nObservations"),
           fill=guide_legend(title="Number of\nObservations"))+
    coord_cartesian(ylim=c(1,-1))+
    facet_grid(NumberObs~.)+ggtitle(main.lab[i])

  plot(p1)
@

<<setup1p0975, echo=FALSE,fig.cap="Results for setup 1 regarding the sensitivity analysis on estimations for the 0.975 quantile.">>=
# p0.975 plot
library(data.table)
library(ggplot2)
  i <- 3
  dat_plot <- subset(res,select=c(sel_var,estim[i]))
  # berechne Veränderungsrate vom estimate über NumberWeights
  setkeyv(dat_plot,c("GROUP_VAR","NumberWeights"))
  dat_plot <- na.omit(dat_plot[,.(EST_CHANGE=diff(get(estim[i])),NumberWeights=NumberWeights[-1]),by=list(GROUP_VAR,GROUP)])
  dat_plot <- dat_plot[,.(Q=c("min","med","max"),quantile(EST_CHANGE,c(0,.5,1))),by=list(GROUP,NumberWeights)]
  dat_plot <- dcast(dat_plot,GROUP+NumberWeights~Q,value.var="V2")
  setnames(dat_plot,c("GROUP"),c("NumberObs"))

  p1 <- ggplot(dat_plot,aes(NumberWeights,med,group=NumberObs))+
    geom_line(aes(color=NumberObs),size=.25)+xlab("Number Bootstrap weights")+ylab("Min - Median - Max")+
    geom_ribbon(aes(ymin = min, ymax = max,fill=NumberObs,colour=NumberObs),linetype = 2, alpha= 0.1)+
    guides(color=guide_legend(title="Number of\nObservations"),
           fill=guide_legend(title="Number of\nObservations"))+
    coord_cartesian(ylim=c(1,-1))+
    facet_grid(NumberObs~.)+ggtitle(main.lab[i])

  plot(p1)
@

Comparing Figures \ref{fig:setup1SD} to \ref{fig:setup1p0975} we conclude that for estimating the standard error a value if 250 bootstrap sample weights does suffice for stable estimates through all group $g$. When it comes to the estimates for the 0.025 and 0.975 quantile the results change drastically. Regardless of the number of observations used for the estimation we suggest to never use less then 500 bootstrap sample weights. But even this can be insufficient when looking at the group $(100,250]$ in our analysis. Ideally one should at least take a 1000 bootstrap sample weights for those kind of estimates.


\subsubsection{Results Setup 2}
For the second setup consider $\hat{\sigma}^{(i,j)}_{(k,g)}$ as $j$-th result, $j=1,\ldots,100$, for the $k$-th estimate, using $i$ bootstrap sample weights, $i\in I$ in group $g$, $g\in G$. We then calculated for fixed $i$, $k$ and $g$ the range over all $\hat{\sigma}^{(i,j)}_{(k,g)}$, $R^{(i)}_{(k,g)}$

\begin{align*}
  R^{(i)}_{(k,g)} = \max(\hat{\sigma}^{(i,j)}_{(k,g)}) - \min(\hat{\sigma}^{(i,j)}_{(k,g)}) \quad,
\end{align*}

From the sets of $R^{(i)}_{(k,g)}$ we then calculated for each $g$ and each $i$ the median, 0.01 and 0.99 quantile.
Figure \ref{fig:setup2SD} displays the results for median, 0.01 and 0.99 quantile for each $g$ over $i$. We followed the same procedure for the estimates regarding 0.025 and 0.975 quantile and the corresponding results are displayed in Figure \ref{fig:setup2p0025} and \ref{fig:setup2p0975}.

<<include=FALSE,cache=TRUE>>=
library(data.table)
library(mountSTAT)
pfad_meth <- mountWinShare("DatenREG","REG_METHODIK","meth")[1]

nB <- seq(50,800,by=50)
res_all <- list()
for(b in nB){
  load(paste0(pfad_meth,"/Gussenbauer/surveysd/Results_boot",b,"_arose.RData"))
  res_boot <- rbindlist(res_boot)
  res_boot[,B:=b]
  res_all <- c(res_all,list(res_boot))
}
res_all <- rbindlist(res_all)

res_all  <- res_all[RB010%in%dat_es[,unique(RB010)]]
res_all[,RB010:=as.numeric(RB010)]

num_obs <- rbindlist(list(dat_es[,.(N=sum(!duplicated(DB030))),by=RB010],dat_es[,.(N=sum(!duplicated(DB030))),by=list(RB010,DB040,DB100)]),use.names=TRUE,fill=TRUE)
num_obs[,GROUP:=cut(N,c(-Inf,50,100,250,500,1000,Inf))]

res_all  <- merge(res_all ,num_obs,by=c("RB010","DB040","DB100"))
res_all[,GROUP_VAR:=.GRP,by=c("RB010","DB040","DB100")]

estim <- c("stE_arose","p0.025_arose","p0.975_arose")
main.lab <- c("Standard Error","0.025 Quantile","0.975 Quantile")
@

<<setup2SD, echo=FALSE,fig.cap="Results for setup 2 regarding the sensitivity analysis on standard error estimations.">>=
# standard error plot
library(data.table)
library(ggplot2)
  i <- 1
  plot_boot <- res_all[,mget(c(estim[i],"GROUP","GROUP_VAR","B"))]
  plot_boot <- plot_boot[,diff(range(get(estim[i]))),by=list(GROUP_VAR,GROUP,B)]
  plot_boot <- plot_boot[,.(Q=c("p.01","p.5","p.99"),quantile(V1,c(.01,.5,.99))),by=list(GROUP,B)]
  plot_boot <- dcast(plot_boot,GROUP+B~Q,value.var="V2")
  setnames(plot_boot,c("GROUP"),c("NumberObs"))

  p1 <- ggplot(plot_boot,aes(factor(B),p.5,group=NumberObs))+
    geom_line(aes(color=NumberObs),size=.25)+xlab("Number Bootstrap weights")+ylab("1% - Median - 99%")+
    geom_ribbon(aes(ymin = p.01, ymax = p.99,fill=NumberObs,colour=NumberObs),linetype = 2, alpha= 0.1)+
    guides(color=guide_legend(title="Number of\nObservations"),
           fill=guide_legend(title="Number of\nObservations"))+
    facet_grid(NumberObs~.)+ggtitle(main.lab[i])

  plot(p1)
@


<<setup2p0025, echo=FALSE,fig.cap="Results for setup 2 regarding the sensitivity analysis on estimations for the 0.025 quantile.">>=
# p0.025 plot
library(data.table)
library(ggplot2)
  i <- 2
  plot_boot <- res_all[,mget(c(estim[i],"GROUP","GROUP_VAR","B"))]
  plot_boot <- plot_boot[,diff(range(get(estim[i]))),by=list(GROUP_VAR,GROUP,B)]
  plot_boot <- plot_boot[,.(Q=c("p.01","p.5","p.99"),quantile(V1,c(.01,.5,.99))),by=list(GROUP,B)]
  plot_boot <- dcast(plot_boot,GROUP+B~Q,value.var="V2")
  setnames(plot_boot,c("GROUP"),c("NumberObs"))

  p1 <- ggplot(plot_boot,aes(factor(B),p.5,group=NumberObs))+
    geom_line(aes(color=NumberObs),size=.25)+xlab("Number Bootstrap weights")+ylab("1% - Median - 99%")+
    geom_ribbon(aes(ymin = p.01, ymax = p.99,fill=NumberObs,colour=NumberObs),linetype = 2, alpha= 0.1)+
    guides(color=guide_legend(title="Number of\nObservations"),
           fill=guide_legend(title="Number of\nObservations"))+
    facet_grid(NumberObs~.)+ggtitle(main.lab[i])

  plot(p1)
@

<<setup2p0975, echo=FALSE,fig.cap="Results for setup 2 regarding the sensitivity analysis on estimations for the 0.975 quantile.">>=
# p0.975 plot
library(data.table)
library(ggplot2)
  i <- 3
  plot_boot <- res_all[,mget(c(estim[i],"GROUP","GROUP_VAR","B"))]
  plot_boot <- plot_boot[,diff(range(get(estim[i]))),by=list(GROUP_VAR,GROUP,B)]
  plot_boot <- plot_boot[,.(Q=c("p.01","p.5","p.99"),quantile(V1,c(.01,.5,.99))),by=list(GROUP,B)]
  plot_boot <- dcast(plot_boot,GROUP+B~Q,value.var="V2")
  setnames(plot_boot,c("GROUP"),c("NumberObs"))

  p1 <- ggplot(plot_boot,aes(factor(B),p.5,group=NumberObs))+
    geom_line(aes(color=NumberObs),size=.25)+xlab("Number Bootstrap weights")+ylab("1% - Median - 99%")+
    geom_ribbon(aes(ymin = p.01, ymax = p.99,fill=NumberObs,colour=NumberObs),linetype = 2, alpha= 0.1)+
    guides(color=guide_legend(title="Number of\nObservations"),
           fill=guide_legend(title="Number of\nObservations"))+
    facet_grid(NumberObs~.)+ggtitle(main.lab[i])

  plot(p1)
@

Looking at Figure \ref{fig:setup2SD} we conclude that for larger groups, e.g above 500 observations, a number of 250 bootstrap sample weights does suffice for getting relatively stable estimates, that is, the variance of the estimator itself is not too large. For groups regarding smaller number of observations it would be beneficial to not use less then 500 observations.\\
Similar to the results from setup 1, the results for the 0.025 and 0.975 quantile, shown in Figure \ref{fig:setup2p0025} and \ref{fig:setup2p0975}, show a very different picture. Even for larger groups one should at least use 500 bootstrap sample weights, although even that might not be enough. As stated for setup one for those kind of estimates it is necessary to use at least 1000 bootstrap sample weights.

\newpage
\bibliography{lib}

\end{document}
